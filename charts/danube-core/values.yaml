# Default values for danube-core.

nameOverride: ""
fullnameOverride: ""

global:
  clusterName: "MY_CLUSTER"

broker:
  replicaCount: 3
  image:
    repository: ghcr.io/danube-messaging/danube-broker
    tag: "v0.7.2"
    pullPolicy: IfNotPresent
  ports:
    client: 6650
    admin: 50051
    prometheus: 9040
  service:
    type: ClusterIP
  externalAccess:
    enabled: false
    type: ClusterIP # ClusterIP | NodePort | LoadBalancer
    # Per-pod broker identity (broker_url) via headless DNS.
    # Rendered by Helm (tpl) then expanded by the K8s container runtime.
    advertisedAddrTemplate: '$(POD_NAME).{{ include "danube-core.broker.headless" . }}.$(POD_NAMESPACE).svc.cluster.local:$(ADVERTISED_PORT)'
    advertisedPort: 6650
    nodePort: null
    # Proxy mode: set connectUrl to the external ingress/proxy address.
    # When set, the broker advertises broker_url (per-pod headless DNS) for internal
    # identity and connect_url (this value) for external clients via the proxy.
    # When empty, both broker_url and connect_url equal the advertisedAddrTemplate (no proxy).
    # Requires broker image with --connect-url CLI flag support.
    connectUrl: ""
  env:
    RUST_LOG: "danube_broker=info,danube_core=info"
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  persistence:
    enabled: true
    size: 10Gi
    storageClass: ""
    mountPath: "/danube-data"
  tls:
    enabled: false
    secretName: ""
    mountPath: "/etc/danube/tls"
  config:
    # Name of an existing ConfigMap that contains danube_broker.yml
    # You must create this ConfigMap before installing the chart.
    existingConfigMap: "danube-broker-config"
    # File name inside the ConfigMap to mount as /etc/danube/<fileName>
    fileName: "danube_broker.yml"
  hpa:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    targetCPU: 70

etcd:
  replicaCount: 1
  image:
    repository: quay.io/coreos/etcd
    tag: "v3.5.9"
    pullPolicy: IfNotPresent
  service:
    port: 2379
    peerPort: 2380
  cluster:
    initialClusterState: "new"
    domain: "cluster.local"
  persistence:
    enabled: true
    size: 2Gi
    storageClass: ""
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  auth:
    enabled: false

prometheus:
  enabled: true
  image:
    repository: prom/prometheus
    tag: "v2.53.0"
    pullPolicy: IfNotPresent
  serviceAccount:
    create: true
    name: ""
  rbac:
    create: true
  service:
    port: 9090
  scrapeInterval: 5s
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  serviceMonitor:
    enabled: false
    interval: 5s
    labels: {}

ingress:
  enabled: false
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
    # Hash-based sticky routing: ensures requests with the same
    # x-danube-broker-url header are routed to the same backend pod.
    nginx.ingress.kubernetes.io/upstream-hash-by: "$http_x_danube_broker_url"
  hosts:
    - host: "broker.example.com"
      paths:
        - path: "/"
          pathType: ImplementationSpecific
          servicePort: "client"
  tls: []
